Problem: gradient descent is slow, each step takes O(nd) time
Optimization 2: stochastic gradient descent

Idea: At each step, pick one misclassified Xi, do gradient descent on loss fn L(Xi*w, yi)
Called the perceptron algorithm, each step takes O(d) time
```
while some yiXi*w < 0:
    w <- w + e yiXi
return w
```

What if the separating hyperplane doesn't pass through the origin?
Add a fictitious dimension, decision function is 
f(x) = w * x + a = [w1 w2 a] * [x1 x2 1]
Allows the bias a to act as a weight

Now, we have sample pts in R^(d+1) d+1 dimensions, all lying on hyperplane x(d+1) = 1
Run perceptron algorithm in (d+1) dimensional space

## Maximum Margin Classifiers

The margin of a linear classifier is the distance from the decision boundary to the nearest sample pt.
What if we make the margin as wide as possible?

Enforce the constraints
yi (w * Xi + a) >= 1 for i in [1,n]
Recall if ||w|| = 1, signed distance from hyperplace to Xi is w * Xi + a
Otherwise, it's w/||w|| * Xi + a/||w||
Hence, the margin(distance from hyperplane to nearest point) is min(1/||W|| * |w*Xi + a|)
To maximize the margin, minimize ||w||. Optimization problem: Find w and a that minimize ||w||^2 subject to yi(xi*w + a) >= 1
Called a quadratic program in d+1 dimensions and n constraints. It has one unique solution!
The solution gives us a maximum margin classifier
