### Small C
More emphasis on margin width
less sensitive to outliers
may underfit simpler boundaries

### Big C
Behaves more like a hard margin, keeps Error small
-more emphasis on respecting the mergin 
more sensitive to outliers
may overfit
more curvy boundaries

## How do we set C?
Use cross validation

## How is epsilon picked?
Epsilon is a parameter the optimization software solves for, the epsilon is always positive. We pass in C

# Features
Given a circle of dots close to the origin and a circle of dots further from the origin, the SVM can't hope to classify these two different classes. Thus, we can use features to transform the data to a linear manner

## Parabolic lifting map
Theorem: I(X1),...,I(Xn) is linearly sepearable iff X1, ..., Xn are seperable by a hypersphere

||X-C||^2 < p^2
||X||^2 + ||C||^2 - 2C*x < p ^2

## Col space
Col space = range = span of the columns of a matrix
Aka all y for which there exists some v s.t. Xv = y

Row space: span of the rows of a matrix
All y for which there exists some v s.t X^Tv = y

## Rank is equivalent to
- Dimensions of the column space of a matrix
- Dimensions of the row space of a matrix
- Number of linearly independent rows
- Number of linearly independent columns

## Invertible
X is invertible if X-1 exists where X^-1 X = X
- Must be a square matrix
- Full rank
- All rows and columns are linearly independent

## Fundamental Theorem of Linear algebra
- A matrix M(mxn) has four fundamental subspaces
    - Rowspace - dimension n
    - Nullspace - dimension n: any vector v s.t. Mv = 0, always include the zero vector
    - Columnspace (aka range col space) - dimension m
    - Left null space (null(A^T))- dimension m
- Rowspace(A) = null(A)
- Colspace(A) = null(A^T)
- Range(A) = span(col(A))

## Vector Calculus

### Why?
- Every technique in this class is to optimize an objective function
- Vector calculus lets us derive algos for optimizations, ie gradient descent, dual SVM formulation, second derivatives
- Most ML problems are in very high dimensional space!

### Review - single variable calculus
Given f(x), the first derivative lets us write a first order linear approximation for f(x) near a given point
- In vector calculus, the derivative of a function is a vector of partial derivatives and is a plane instead of a line

### Differentiation from First Principles
- For a first order approximation, we ignore everything higher than the first derivative
- derivatives = a transposed gradient
- You can figure out almost all of the gradients you need for 189 just by knowning
    - delta(Ax) = A^T <- matrix multiplied by vector
    - delta(w^Tx) = w^T <- vector multiplied by vector
    - delta(x^t * Ax) = (A+ A^T)x <- matrix multiplied by two vectors - quadratic form
- differentiation from first principles works, you can always do it via the direct way